import os
import pandas as pd
from jinja2 import Environment, FileSystemLoader
from playwright.sync_api import sync_playwright
import math
import shutil
from PIL import Image, ImageFilter, ImageDraw, ImageFont
import re
import warnings
import subprocess
import sys

# --- 1. T·ª∞ ƒê·ªòNG C√ÄI ƒê·∫∂T PLAYWRIGHT & DEPENDENCIES ---
def ensure_playwright_installed():
    """
    H√†m n√†y ch·∫°y khi kh·ªüi ƒë·ªông ƒë·ªÉ ƒë·∫£m b·∫£o m√¥i tr∆∞·ªùng Playwright ƒë√£ s·∫µn s√†ng.
    N√≥ kh·∫Øc ph·ª•c l·ªói 'Host system is missing dependencies'.
    """
    print("‚¨áÔ∏è ƒêang ki·ªÉm tra v√† c√†i ƒë·∫∑t m√¥i tr∆∞·ªùng Playwright...")
    try:
        # B∆∞·ªõc 1: C√†i ƒë·∫∑t tr√¨nh duy·ªát Chromium
        print("   -> ƒêang c√†i ƒë·∫∑t tr√¨nh duy·ªát Chromium...")
        subprocess.check_call([sys.executable, "-m", "playwright", "install", "chromium"])
        
        # B∆∞·ªõc 2: C√†i ƒë·∫∑t dependencies h·ªá th·ªëng (Ch·ªâ ch·∫°y tr√™n Linux/Mac)
        # L·ªánh n√†y t∆∞∆°ng ƒë∆∞∆°ng v·ªõi 'sudo playwright install-deps'
        if os.name == 'posix':
            print("   -> ƒêang c√†i ƒë·∫∑t th∆∞ vi·ªán h·ªá th·ªëng (install-deps)...")
            try:
                # L∆∞u √Ω: L·ªánh n√†y c√≥ th·ªÉ y√™u c·∫ßu quy·ªÅn sudo/root. 
                # N·∫øu ch·∫°y trong Docker/Replit container th∆∞·ªùng ƒë√£ c√≥ quy·ªÅn ho·∫∑c c·∫ßn c·∫•u h√¨nh ri√™ng.
                subprocess.call([sys.executable, "-m", "playwright", "install-deps"], 
                                stderr=subprocess.DEVNULL, stdout=subprocess.DEVNULL)
            except Exception as e:
                print(f"      (Th√¥ng b√°o: Kh√¥ng th·ªÉ ch·∫°y install-deps t·ª± ƒë·ªông: {e}. N·∫øu g·∫∑p l·ªói, h√£y ch·∫°y th·ªß c√¥ng.)")
                
        print("‚úÖ M√¥i tr∆∞·ªùng Playwright ƒë√£ s·∫µn s√†ng.")
    except Exception as e:
        print(f"‚ö†Ô∏è C·∫£nh b√°o: L·ªói khi c√†i ƒë·∫∑t t·ª± ƒë·ªông. Chi ti·∫øt: {e}")
        print("   Vui l√≤ng ch·∫°y th·ªß c√¥ng: 'playwright install' v√† 'playwright install-deps'")

# G·ªçi h√†m c√†i ƒë·∫∑t ngay l·∫≠p t·ª©c
ensure_playwright_installed()


# --- T·∫ÆT C·∫¢NH B√ÅO PANDAS ---
warnings.simplefilter(action='ignore', category=FutureWarning)

# --- C·∫§U H√åNH ---
EXCEL_DIR = "excel_files"
HTML_DIR = "output_html"
IMAGE_DIR = "output_images"
DATA_CHECK_DIR = "data_check"

TARGET_WIDTH, TARGET_HEIGHT = 2000, 1300
ROW_HEIGHT = 108
ROWS_PER_PAGE = 10
DATA_COLS_TO_KEEP = 15
INDEX_COL_WIDTH = 50
TOP_3_COL_WIDTH = 200
OTHER_COL_WIDTH = 105
AVG_CHAR_WIDTH = 8.5
CELL_PADDING_X = 10
LINE_HEIGHT_ESTIMATE = 18

# --- C·∫§U H√åNH GI·ªöI H·∫†N LEAK ---
MAX_LEAK_TOLERANCE = 5  # Cho ph√©p l·ªçt t·ªëi ƒëa 5 l·ªói/file

# --- C·∫§U H√åNH WATERMARK ---
WATERMARK_TEXT = "DATALD.COM"
WATERMARK_OPACITY = 90  # ƒê·ªô ƒë·∫≠m nh·∫°t (0-255)
GRID_COLS = 3 
GRID_ROWS = 3 

# --- DANH S√ÅCH T·ª™ KH√ìA C·∫¶N M√É H√ìA (Case-insensitive) ---
KEYWORDS_TO_MASK = [
    "trangvang", 
    "scribd", 
    "hsct", 
    "hosocongty", 
    "thu·∫ø",
    "thue",
    "masothue", 
    "data5s", 
    "google.com/maps/"
]

# --- H√ÄM M√É H√ìA (MASKING) ---
def mask_all_chars(match):
    """Callback: Thay th·∫ø to√†n b·ªô chu·ªói kh·ªõp b·∫±ng d·∫•u *"""
    return '*' * len(match.group())

def mask_number_group(match):
    """Callback: M√£ h√≥a 50% ƒë·ªô d√†i c·ªßa c·ª•m s·ªë"""
    digit_str = match.group()
    length = len(digit_str)
    num_to_mask = math.ceil(length / 2)
    return ("*" * num_to_mask) + digit_str[num_to_mask:]

def mask_email_group(match):
    """Callback: M√£ h√≥a user c·ªßa email"""
    email = match.group()
    if '@' in email:
        user_part, domain_part = email.split('@', 1)
        masked_user = '*' * len(user_part)
        return f"{masked_user}@{domain_part}"
    return email

def mask_cell_value(value):
    """
    H√†m m√£ h√≥a trung t√¢m.
    Th·ª© t·ª± ∆∞u ti√™n: URL -> T·ª´ kh√≥a -> Email -> S·ªë
    """
    text = str(value)
    if not text or text.strip().lower() == 'nan':
        return ""
    
    # --- 1. M√É H√ìA URL (HTTPS/WWW) ---
    # Regex t√¨m chu·ªói b·∫Øt ƒë·∫ßu b·∫±ng http://, https:// ho·∫∑c www.
    # \S+ nghƒ©a l√† l·∫•y c√°c k√Ω t·ª± li√™n ti·∫øp kh√¥ng ph·∫£i kho·∫£ng tr·∫Øng
    url_pattern = r'\b(?:https?://|www\.)\S+\b'
    text = re.sub(url_pattern, mask_all_chars, text)

    # --- 2. M√É H√ìA T·ª™ KH√ìA ƒê·∫∂C BI·ªÜT ---
    # Duy·ªát qua danh s√°ch t·ª´ kh√≥a v√† m√£ h√≥a to√†n b·ªô
    for keyword in KEYWORDS_TO_MASK:
        # re.escape ƒë·ªÉ x·ª≠ l√Ω k√Ω t·ª± ƒë·∫∑c bi·ªát nh∆∞ d·∫•u ch·∫•m, g·∫°ch ch√©o
        # re.IGNORECASE ƒë·ªÉ kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng
        pattern = re.compile(re.escape(keyword), re.IGNORECASE)
        text = pattern.sub(mask_all_chars, text)

    # --- 3. M√É H√ìA EMAIL ---
    email_pattern = r'\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}\b'
    text = re.sub(email_pattern, mask_email_group, text)

    # --- 4. M√É H√ìA S·ªê ---
    text = re.sub(r'\d+', mask_number_group, text)
    
    return text

# --- H√ÄM KI·ªÇM TRA B·∫¢O M·∫¨T ---
def check_html_leakage_count(html_path, file_name_check):
    leak_count = 0
    details = []
    try:
        with open(html_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Check Email
        email_pattern = r'\b[a-zA-Z0-9][a-zA-Z0-9._%+-]*@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}\b'
        leaked_emails = re.findall(email_pattern, content)
        if leaked_emails:
            leak_count += len(leaked_emails)
            details.extend(leaked_emails)

        # Check S·ªë
        number_pattern = r'(?<!\*)\b\d{7,}\b'
        leaked_numbers = re.findall(number_pattern, content)
        if leaked_numbers:
            for num in leaked_numbers:
                if num in file_name_check: continue
                leak_count += 1
                details.append(num)
            
        return leak_count, details
    except Exception as e:
        print(f"L·ªói check file {html_path}: {e}")
        return 1, ["L·ªói ƒë·ªçc file"]

# --- C√ÅC H√ÄM H·ªñ TR·ª¢ D·ªÆ LI·ªÜU ---
def get_column_widths(df_chunk):
    if df_chunk.empty: return []
    try:
        avg_lengths = df_chunk.astype(str).map(lambda x: len(str(x).strip())).mean()
    except AttributeError:
        avg_lengths = df_chunk.astype(str).applymap(lambda x: len(str(x).strip())).mean()
    top_3_indices = avg_lengths.sort_values(ascending=False).head(3).index
    data_widths = []
    for i in range(len(df_chunk.columns)):
        if i in top_3_indices: data_widths.append(TOP_3_COL_WIDTH)
        else: data_widths.append(OTHER_COL_WIDTH)
    return [INDEX_COL_WIDTH] + data_widths

def get_cell_class(text, col_width):
    text_space = col_width - CELL_PADDING_X
    if text_space <= 0: return "no-wrap-text"
    num_lines = math.ceil(len(str(text).strip()) * AVG_CHAR_WIDTH / text_space)
    estimated_height = num_lines * LINE_HEIGHT_ESTIMATE
    return "wrap-text" if estimated_height <= ROW_HEIGHT else "no-wrap-text"

# --- CORE: T·∫†O HTML ---
def generate_html_for_sheet(df, file_name, sheet_name, template):
    generated_files = [] 
    
    if len(df.columns) > DATA_COLS_TO_KEEP: df = df.iloc[:, :DATA_COLS_TO_KEEP]
    df = df[df.apply(lambda row: any(str(cell).strip() != '' and str(cell).strip().lower() != 'nan' for cell in row), axis=1)]
    if df.empty: return []

    # --- CH·ªà T·∫†O C√ÅC TRANG N·ªòI DUNG (PAGES) ---
    num_pages = math.ceil(len(df) / ROWS_PER_PAGE)
    for i in range(num_pages):
        page_df = df.iloc[i*ROWS_PER_PAGE : (i+1)*ROWS_PER_PAGE]
        temp_df = page_df.copy()
        while len(temp_df.columns) < DATA_COLS_TO_KEEP: temp_df[f'ph_{len(temp_df.columns)}'] = ''
        col_widths = get_column_widths(temp_df)

        page_data = []
        for r_idx, row in page_df.iterrows():
            row_cells = []
            for c_idx, cell in enumerate(row):
                val = mask_cell_value('' if str(cell).strip().lower() == 'nan' else cell)
                row_cells.append({"value": val, "class": get_cell_class(val, col_widths[c_idx+1])})
            while len(row_cells) < DATA_COLS_TO_KEEP: row_cells.append({"value": "", "class": "wrap-text"})
            page_data.append({"excel_row_num": r_idx + 1, "cells": row_cells})
        while len(page_data) < ROWS_PER_PAGE:
            page_data.append({"excel_row_num": " ", "cells": [{"value": "", "class": "wrap-text"}]*DATA_COLS_TO_KEEP})

        page_html_path = os.path.join(HTML_DIR, f"{file_name}_{sheet_name}_page_{i+1}.html")
        with open(page_html_path, 'w', encoding='utf-8') as f:
            f.write(template.render({"title": f"{file_name} - {sheet_name} - P{i+1}", "page_data": page_data, "column_widths": col_widths}))
        generated_files.append((f"PAGE_{i+1}", page_html_path, sheet_name))
        
    return generated_files

# --- H√ÄM X·ª¨ L√ù WATERMARK ---
def add_watermark_to_image(image_path):
    """
    Th√™m watermark 'datald.com' m·ªù v√†o ·∫£nh.
    B·ªë c·ª•c: L∆∞·ªõi 3 c·ªôt x 3 h√†ng.
    """
    try:
        # M·ªü ·∫£nh d∆∞·ªõi d·∫°ng RGBA ƒë·ªÉ x·ª≠ l√Ω trong su·ªët
        base_image = Image.open(image_path).convert("RGBA")
        width, height = base_image.size
        
        # T·∫°o m·ªôt layer trong su·ªët ƒë·ªÉ v·∫Ω ch·ªØ
        txt_layer = Image.new("RGBA", base_image.size, (255, 255, 255, 0))
        draw = ImageDraw.Draw(txt_layer)
        
        # C√†i ƒë·∫∑t Font ch·ªØ
        font_size = int(width / 25) 
        try:
            # ∆Øu ti√™n font Arial ho·∫∑c DejaVu n·∫øu c√≥
            font_path = "arial.ttf"
            if not os.path.exists(font_path) and os.name == 'posix':
                font_path = "/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf"
            font = ImageFont.truetype(font_path, font_size)
        except IOError:
            font = ImageFont.load_default()
        
        # T√≠nh to√°n l∆∞·ªõi
        step_x = width / GRID_COLS
        step_y = height / GRID_ROWS
        
        # M√†u ch·ªØ: X√°m nh·∫°t + Opacity
        text_color = (150, 150, 150, WATERMARK_OPACITY)
        
        for r in range(GRID_ROWS):
            for c in range(GRID_COLS):
                center_x = (c * step_x) + (step_x / 2)
                center_y = (r * step_y) + (step_y / 2)
                
                bbox = draw.textbbox((0, 0), WATERMARK_TEXT, font=font)
                text_w = bbox[2] - bbox[0]
                text_h = bbox[3] - bbox[1]
                
                x = center_x - (text_w / 2)
                y = center_y - (text_h / 2)
                
                draw.text((x, y), WATERMARK_TEXT, font=font, fill=text_color)
        
        combined = Image.alpha_composite(base_image, txt_layer)
        combined = combined.convert("RGB")
        combined.save(image_path)
        
    except Exception as e:
        print(f"     ‚ö†Ô∏è L·ªói th√™m watermark: {e}")

# --- H√ÄM T·∫†O ·∫¢NH T·ª™ LIST HTML ---
def create_images_from_list(html_list, file_name):
    print(f"     ‚úÖ FILE H·ª¢P L·ªÜ. ƒêang t·∫°o {len(html_list)} ·∫£nh...")
    with sync_playwright() as p:
        # C·∫•u h√¨nh launch: "--no-sandbox" gi√∫p ch·∫°y tr√™n c√°c m√¥i tr∆∞·ªùng CI/Server/Replit
        browser = p.chromium.launch(args=["--no-sandbox", "--disable-setuid-sandbox"])
        page = browser.new_page()
        page.set_viewport_size({"width": TARGET_WIDTH, "height": TARGET_HEIGHT})
        
        for type_label, html_path, sheet_name in html_list:
            sheet_output_dir = os.path.join(IMAGE_DIR, file_name, sheet_name)
            os.makedirs(sheet_output_dir, exist_ok=True)

            img_filename = os.path.basename(html_path).replace(".html", ".png")
            img_path = os.path.join(sheet_output_dir, img_filename)
            
            # 1. Ch·ª•p ·∫£nh m√†n h√¨nh
            page.goto(f"file://{os.path.abspath(html_path)}")
            page.screenshot(path=img_path, full_page=False)
            
            # 2. Th√™m Watermark
            add_watermark_to_image(img_path)

        browser.close()

def cleanup_htmls(html_list):
    for _, path, _ in html_list:
        if os.path.exists(path):
            os.remove(path)

# --- WORKFLOW CH√çNH ---
def process_excel_file_workflow(excel_path, template):
    file_name = os.path.splitext(os.path.basename(excel_path))[0]
    print(f"\n--- FILE: {os.path.basename(excel_path)} ---")
    
    try:
        all_sheets = pd.read_excel(excel_path, sheet_name=None, header=None)
    except Exception as e:
        print(f"L·ªói ƒë·ªçc file: {e}")
        return

    def generate_all_htmls():
        total_htmls = []
        for sheet_name, df in all_sheets.items():
            sheet_htmls = generate_html_for_sheet(df, file_name, sheet_name, template)
            total_htmls.extend(sheet_htmls)
        return total_htmls

    # 1. T·∫°o HTML
    print(f"     1Ô∏è‚É£  Giai ƒëo·∫°n 1: T·∫°o HTML to√†n b·ªô c√°c sheet...")
    all_html_files = generate_all_htmls()
    if not all_html_files:
        print("     ‚ö†Ô∏è  File kh√¥ng c√≥ d·ªØ li·ªáu h·ª£p l·ªá.")
        return

    # 2. Check L·∫ßn 1
    print(f"     2Ô∏è‚É£  Giai ƒëo·∫°n 2: Qu√©t l·ªói b·∫£o m·∫≠t (Ng∆∞·ª°ng cho ph√©p: {MAX_LEAK_TOLERANCE} l·ªói)...")
    total_leaks = 0
    all_leak_details = []
    
    for _, html_path, _ in all_html_files:
        count, details = check_html_leakage_count(html_path, file_name)
        total_leaks += count
        all_leak_details.extend(details)
    
    should_retry = False
    if total_leaks > MAX_LEAK_TOLERANCE:
        print(f"     ‚ö†Ô∏è  C·∫£nh b√°o: Ph√°t hi·ªán {total_leaks} l·ªói. V√≠ d·ª•: {all_leak_details[:3]}...")
        print("     üîÑ ƒêang th·ª≠ t·∫°o l·∫°i (Retry to√†n b·ªô file)...")
        should_retry = True
    elif total_leaks > 0:
        print(f"     ‚ö†Ô∏è  Th√¥ng b√°o: Ph√°t hi·ªán {total_leaks} l·ªói (Trong ng∆∞·ª°ng cho ph√©p). Ti·∫øp t·ª•c.")
    else:
        print(f"     ‚úÖ Tuy·ªát v·ªùi: Kh√¥ng ph√°t hi·ªán l·ªói n√†o.")

    # 3. Retry n·∫øu c·∫ßn
    if should_retry:
        cleanup_htmls(all_html_files)
        all_html_files = generate_all_htmls()
        total_leaks = 0
        all_leak_details = []
        for _, html_path, _ in all_html_files:
            count, details = check_html_leakage_count(html_path, file_name)
            total_leaks += count
            all_leak_details.extend(details)
            
    # 4. Quy·∫øt ƒë·ªãnh cu·ªëi c√πng
    if total_leaks > MAX_LEAK_TOLERANCE:
        print(f"     ‚ùå STOP: V·∫´n c√≤n {total_leaks} l·ªói sau khi retry.")
        dest_path = os.path.join(DATA_CHECK_DIR, os.path.basename(excel_path))
        if not os.path.exists(dest_path):
            shutil.copy2(excel_path, dest_path)
            print(f"     -> ƒê√£ copy file Excel v√†o '{DATA_CHECK_DIR}'")
        else:
            print(f"     -> File Excel ƒë√£ t·ªìn t·∫°i trong '{DATA_CHECK_DIR}'")
        cleanup_htmls(all_html_files)
        print("     üö´ ƒê√£ h·ªßy b·ªè output cho to√†n b·ªô file n√†y.")
        return 
    
    else:
        try:
            create_images_from_list(all_html_files, file_name)
        except Exception as e:
            print(f"     ‚ùå L·ªói khi t·∫°o ·∫£nh: {e}")
        finally:
            cleanup_htmls(all_html_files)

# --- MAIN ---
def main():
    print(f"üöÄ B·∫Øt ƒë·∫ßu... (Masking URLs/Keywords + Auto-Install Browser)")
    os.makedirs(HTML_DIR, exist_ok=True)
    os.makedirs(IMAGE_DIR, exist_ok=True)
    os.makedirs(DATA_CHECK_DIR, exist_ok=True)

    env = Environment(loader=FileSystemLoader('.'))
    template = env.get_template('template.html')

    excel_files = [f for f in os.listdir(EXCEL_DIR) if f.endswith(('.xlsx', '.xls'))]
    if not excel_files:
        print("Kh√¥ng t√¨m th·∫•y file Excel.")
        return

    for filename in excel_files:
        excel_path = os.path.join(EXCEL_DIR, filename)
        process_excel_file_workflow(excel_path, template)

    if os.path.exists(HTML_DIR):
        try: shutil.rmtree(HTML_DIR)
        except: pass
        
    print("\n‚úÖ HO√ÄN T·∫§T. Vui l√≤ng ki·ªÉm tra th∆∞ m·ª•c 'data_check'.")

if __name__ == "__main__":
    main()